{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db90be3-9913-4b5c-9f95-c91a9808fc6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install repeng accelerate datasets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a04e8d3-b8f9-476e-9fe5-848e4b2da034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"To copy construct from a tensor\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0c3f45-3813-4d48-8f2c-9020b5695b6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RASPID with dynamic PID-steering:\n",
    "- Train chunk-level classifier and control vector on the first 1000 labeled chains\n",
    "- Hold out the last 200 chains for final GSM8K evaluation only\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from repeng import ControlModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ─── 1) CONFIG & LOAD ──────────────────────────────────────────────────────\n",
    "\n",
    "MODEL_NAME    = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE         = torch.float32\n",
    "\n",
    "LABELED_CSV   = \"gsm8k_chains_labeled_with_tokens.csv\"\n",
    "CTRL_VEC_PATH = \"ctrl_vector.pt\"\n",
    "\n",
    "# classifier hyperparams\n",
    "EMB_LAYER     = 20\n",
    "CHUNK_SIZES   = [16,24]\n",
    "BATCH_SIZE    = 32\n",
    "FLUFF_STAR    = 0.5   # target probability for “redundant”\n",
    "\n",
    "# PID steering hyperparams\n",
    "INIT_FREE     = 40\n",
    "STEER_WINDOW  = 60\n",
    "KP, KI, KD    = 0.05, 0.001, 0.001\n",
    "MAX_I, DERIV  = 0.20, 0.01\n",
    "MAX_ALPHA     = 0.40\n",
    "BASE_TEMP     = 0.70\n",
    "STEER_TEMP    = 0.20\n",
    "MAX_REPEAT    = 8\n",
    "\n",
    "# load tokenizer & models\n",
    "tokenizer    = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "base_model   = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None\n",
    ").eval()\n",
    "control_model = ControlModel(base_model, [EMB_LAYER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8971b2b-e39d-4728-8b9f-6ac4c64568c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ─── 2) SPLIT LABELED DATA ─────────────────────────────────────────────────\n",
    "\n",
    "df_all = pd.read_csv(LABELED_CSV)\n",
    "\n",
    "# first 1000 for training classifier & control vector\n",
    "df_ctrl = df_all.iloc[:1000]\n",
    "# last 200 reserved for later (but not used to train classifier)\n",
    "df_eval = df_all.iloc[1000:1200]\n",
    "\n",
    "required_ctrl  = df_ctrl[\"required_thoughts\"].fillna(\"\")\n",
    "redundant_ctrl = df_ctrl[\"redundant_thoughts\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9af953e-37c6-4003-a947-fd32cd6fa083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec49470535bf442f9d01fd338ec4036d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b40dbd6de3e4bfb8afc726b6d0797d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training clf @ cs=16:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=16 → val_acc=0.295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476f96db20ad4e459a29e577d2398f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1420a7ea268f4ff2bb760b4dd9363d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training clf @ cs=24:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=24 → val_acc=0.722\n",
      "✔ Selected chunk_size=24, val_acc=0.722\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Disable HuggingFace tokenizer parallel warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, texts, label, cs):\n",
    "        self.cs = cs\n",
    "        self.chunks = []\n",
    "        self.labels = []\n",
    "        for t in texts:\n",
    "            tok_ids = tokenizer.encode(t, add_special_tokens=False)\n",
    "            for i in range(0, len(tok_ids) - cs + 1, cs):\n",
    "                self.chunks.append(tok_ids[i : i + cs])\n",
    "                self.labels.append(label)\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunks[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "    # pad on CPU so pin_memory works\n",
    "    seqs = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "    padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        seqs, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask = (padded != tokenizer.pad_token_id).long()\n",
    "    return {\"input_ids\": padded, \"attention_mask\": attention_mask}, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "best_cs, best_acc, best_clf = None, 0.0, None\n",
    "\n",
    "for cs in CHUNK_SIZES:\n",
    "    # build datasets\n",
    "    ds0 = ChunkDataset(required_ctrl, 0, cs)\n",
    "    ds1 = ChunkDataset(redundant_ctrl, 1, cs)\n",
    "    loader = DataLoader(\n",
    "        ConcatDataset([ds0, ds1]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # no multiprocessing to avoid fork issues\n",
    "    )\n",
    "\n",
    "    # embed all chunks\n",
    "    feats, labels = [], []\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens, batch_labels in tqdm(loader):\n",
    "            # move to GPU\n",
    "            batch_tokens = {\n",
    "                k: v.to(DEVICE, non_blocking=True)\n",
    "                for k, v in batch_tokens.items()\n",
    "            }\n",
    "            out = base_model(**batch_tokens, output_hidden_states=True)\n",
    "            h = out.hidden_states[EMB_LAYER].mean(1)\n",
    "            feats.append(h.cpu().numpy())\n",
    "            labels.append(batch_labels.numpy())\n",
    "    X = np.vstack(feats)\n",
    "    y = np.concatenate(labels)\n",
    "\n",
    "    # train/validation split\n",
    "    Xtr, Xval, ytr, yval = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # SGD training with tqdm progress\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"log_loss\", random_state=42, warm_start=True, max_iter=1, tol=None\n",
    "    )\n",
    "    prev_coef = None\n",
    "    pbar = tqdm(range(500), desc=f\"Training clf @ cs={cs}\", leave=False)\n",
    "    for _ in pbar:\n",
    "        clf.fit(Xtr, ytr)\n",
    "        coef = clf.coef_\n",
    "        if prev_coef is not None:\n",
    "            delta = np.max(np.abs(coef - prev_coef))\n",
    "            pbar.set_postfix(delta=delta)\n",
    "            if delta < 1e-3:\n",
    "                break\n",
    "        prev_coef = coef.copy()\n",
    "    pbar.close()\n",
    "\n",
    "    acc = accuracy_score(yval, clf.predict(Xval))\n",
    "    print(f\"chunk_size={cs} → val_acc={acc:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_cs, best_acc, best_clf = cs, acc, clf\n",
    "\n",
    "print(f\"✔ Selected chunk_size={best_cs}, val_acc={best_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8eb6e6-1984-4c4b-b256-104daf0ffbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3108e6ada9f54981b674311e5b4f84d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea201cb4c7c46549a355e930ef52b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved control vector\n"
     ]
    }
   ],
   "source": [
    "# ─── 4) BUILD CONTROL VECTOR FROM CTRL SET ────────────────────────────────\n",
    "\n",
    "def mean_hidden(texts):\n",
    "    vs = []\n",
    "    for t in tqdm(texts):\n",
    "        toks = tokenizer(t, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "        with torch.inference_mode():\n",
    "            h = base_model(**toks, output_hidden_states=True).hidden_states[EMB_LAYER][0]\n",
    "        vs.append(h.mean(0).cpu())\n",
    "    return torch.stack(vs).mean(0)\n",
    "\n",
    "v_req = mean_hidden(required_ctrl)\n",
    "v_red = mean_hidden(redundant_ctrl)\n",
    "ctrl_vec = {EMB_LAYER: (v_req - v_red).to(DEVICE)}\n",
    "torch.save(ctrl_vec, CTRL_VEC_PATH)\n",
    "print(\"✅ Saved control vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e561d1ef-8610-46a6-9e1e-d4c8a39850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from repeng import ControlVector\n",
    "\n",
    "# model_type is a short string identifying your model, e.g. \"qwen\" or whatever base_model.config.model_type gives\n",
    "model_type = base_model.config.model_type  # e.g. \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "ctrl_vec = ControlVector(\n",
    "    model_type = model_type,\n",
    "    directions={EMB_LAYER: (v_req - v_red).to(DEVICE)}\n",
    ")\n",
    "\n",
    "# now you can save it\n",
    "torch.save(ctrl_vec, \"ctrl_vector.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da85956c-af77-4c2c-9fb4-5fd728b2d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688d6cf0-2047-41b2-8165-c16e418e9a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_baseline(prompt, max_new_tokens=MAX_TOKENS):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = base_model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True, temperature=0.6,\n",
    "        top_p=0.9, repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    toks = out.shape[1] - inp.input_ids.shape[1]\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True), toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd647c6e-0d84-45b0-8441-61d933b00e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuned hyper-params\n",
    "INIT_FREE      = 80          # let the model reason first\n",
    "STEER_WINDOW   = 60\n",
    "BASE_TEMP      = 0.60        # same as baseline\n",
    "STEER_TEMP     = 0.30\n",
    "STEER_MARGIN   = 0.20        # p_red must exceed 0.5+0.2\n",
    "MAX_ALPHA      = 0.40\n",
    "MAX_RAW        = 50.0        # clamp for exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc595974-1a46-4b1e-b5a4-273f11afcc56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_raspid(prompt, max_new_tokens=MAX_TOKENS, debug=True):\n",
    "    \"\"\"\n",
    "    Fixed RASPID generation function that properly applies the control vector\n",
    "    and handles numerical stability issues.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n=== RASPID GENERATION ===\")\n",
    "        print(f\"Original prompt: '{prompt}'\")\n",
    "        print(f\"MAX_TOKENS: {MAX_TOKENS}, INIT_FREE: {INIT_FREE}, STEER_WINDOW: {STEER_WINDOW}\")\n",
    "        print(f\"Temperatures: BASE_TEMP: {BASE_TEMP}, STEER_TEMP: {STEER_TEMP}\")\n",
    "        print(f\"PID: KP={KP}, KI={KI}, KD={KD}, MAX_I={MAX_I}, MAX_ALPHA={MAX_ALPHA}\")\n",
    "        print(f\"FLUFF_STAR: {FLUFF_STAR}, EMB_LAYER: {EMB_LAYER}\")\n",
    "    \n",
    "    # For structured control vectors (like ControlVector with directions dict),\n",
    "    # we should leave the structure intact\n",
    "    ctrl_vec_normalized = ctrl_vec\n",
    "\n",
    "    # Setup generation\n",
    "    stop_re = re.compile(r\"\\\\boxed\\{[^{}]{1,12}\\}\")\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE).input_ids[0]\n",
    "    out_ids = ids.clone()\n",
    "    past = None\n",
    "    alpha = I = D = prev_err = 0.0\n",
    "    chunk_h = None\n",
    "    tok_in_chunk = 0\n",
    "    steering = False\n",
    "    steer_start = 0\n",
    "    last_tok = None\n",
    "    rep_ctr = 0\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n--- RASPID TRACE ---\")\n",
    "        print(\"step | on | p_red |  err  |   α   |   I   |   D   | temp | token\")\n",
    "    \n",
    "    pbar = tqdm(range(max_new_tokens), desc=\"RASPID gen\", leave=False)\n",
    "    for step in pbar:\n",
    "        gen_len = out_ids.size(0) - ids.size(0)\n",
    "        \n",
    "        # Check if steering should be activated/deactivated\n",
    "        if not steering and gen_len >= INIT_FREE:\n",
    "            steering, steer_start = True, gen_len\n",
    "            if debug:\n",
    "                print(f\"[INFO] Activating steering at gen_len={gen_len}\")\n",
    "        if steering and gen_len - steer_start > STEER_WINDOW:\n",
    "            if debug:\n",
    "                print(f\"[INFO] Deactivating steering at gen_len={gen_len}\")\n",
    "            steering, alpha, I, D = False, 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Get coefficient and apply control\n",
    "        coeff = alpha if steering else 0.0\n",
    "        control_model.set_control(ctrl_vec_normalized, coeff=coeff)\n",
    "        \n",
    "        # CRITICAL FIX: Process the entire prompt on the first step\n",
    "        if gen_len == 0 and step == 0:\n",
    "            # First step - process the entire prompt\n",
    "            out = control_model(\n",
    "                input_ids=out_ids.unsqueeze(0),  # Use the full prompt\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            # Subsequent steps - process only the new token with the cached state\n",
    "            out = control_model(\n",
    "                input_ids=out_ids[-1:].unsqueeze(0),  # Only the last token\n",
    "                past_key_values=past,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        \n",
    "        past, logits = out.past_key_values, out.logits[0, -1]\n",
    "        h_last = out.hidden_states[EMB_LAYER][0, -1]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if torch.isnan(logits).any():\n",
    "            logits = torch.nan_to_num(logits)\n",
    "        if torch.isnan(h_last).any():\n",
    "            h_last = torch.nan_to_num(h_last)\n",
    "        \n",
    "        # Check for token repetition\n",
    "        tok = out_ids[-1].item()\n",
    "        if tok == last_tok:\n",
    "            rep_ctr += 1\n",
    "            if rep_ctr >= MAX_REPEAT:\n",
    "                if debug:\n",
    "                    print(f\"[INFO] Hit MAX_REPEAT={MAX_REPEAT}, stopping generation\")\n",
    "                break\n",
    "        else:\n",
    "            rep_ctr, last_tok = 0, tok\n",
    "        \n",
    "        # Normalize hidden states for stability\n",
    "        h_last_norm = h_last / (torch.norm(h_last) + 1e-8)\n",
    "        \n",
    "        # Track hidden states with normalized values\n",
    "        chunk_h = h_last_norm if chunk_h is None else chunk_h + h_last_norm\n",
    "        tok_in_chunk += 1\n",
    "        \n",
    "        # Classifier and PID controller\n",
    "        p_red = err = 0.0\n",
    "        if tok_in_chunk >= best_cs:\n",
    "            try:\n",
    "                # Use normalized chunk_h for classifier\n",
    "                classifier_input = (chunk_h / best_cs).cpu().unsqueeze(0).numpy()\n",
    "                \n",
    "                # Check for NaN values\n",
    "                if np.isnan(classifier_input).any():\n",
    "                    classifier_input = np.nan_to_num(classifier_input)\n",
    "                \n",
    "                # Get raw classifier output\n",
    "                raw = best_clf.decision_function(classifier_input)[0]\n",
    "                \n",
    "                # Scale down extreme classifier values\n",
    "                if abs(raw) > 50.0:\n",
    "                    scaled_raw = 50.0 * (np.sign(raw) * np.log(1 + abs(raw) / 50.0) / np.log(1 + abs(raw) / 50.0 * 20))\n",
    "                    if debug:\n",
    "                        print(f\"[INFO] Scaling down extreme classifier value: {raw:.2f} -> {scaled_raw:.2f}\")\n",
    "                    raw = scaled_raw\n",
    "                else:\n",
    "                    raw = max(-50.0, min(50.0, raw))\n",
    "                \n",
    "                p_red = 1.0 / (1.0 + math.exp(-raw))\n",
    "                \n",
    "                if p_red > FLUFF_STAR + 0.20:\n",
    "                    err = p_red - FLUFF_STAR\n",
    "                    \n",
    "                    # Update PID controller\n",
    "                    I = max(-MAX_I, min(MAX_I, I + KI * err))\n",
    "                    D = KD * (err - prev_err) + (1 - KD) * D\n",
    "                    prev_err = err\n",
    "                    alpha = max(0.0, min(MAX_ALPHA, alpha + KP * err + I + D))\n",
    "                \n",
    "                # Reset chunk\n",
    "                chunk_h = None\n",
    "                tok_in_chunk = 0\n",
    "                \n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[ERROR] Error in classifier/PID section: {e}\")\n",
    "                chunk_h = h_last_norm\n",
    "                tok_in_chunk = 1\n",
    "        \n",
    "        # Temperature and sampling\n",
    "        temp = BASE_TEMP * (1 - coeff / MAX_ALPHA) + STEER_TEMP * (coeff / MAX_ALPHA)\n",
    "        \n",
    "        # Apply temperature and get probabilities\n",
    "        try:\n",
    "            logits_safe = logits.clamp(-100, 100)\n",
    "            probs = torch.softmax(logits_safe / temp, dim=-1)\n",
    "            \n",
    "            if torch.isnan(probs).any():\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "            \n",
    "            # Sample token\n",
    "            nxt = torch.multinomial(probs, 1).item()\n",
    "            token_str = tokenizer.decode([nxt], skip_special_tokens=True).replace(\"\\n\",\"\\\\n\")\n",
    "            \n",
    "            # Add token to output\n",
    "            generated_text += token_str\n",
    "            out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"[ERROR] Error in sampling: {e}\")\n",
    "            # Fallback to argmax sampling\n",
    "            nxt = torch.argmax(logits).item()\n",
    "            token_str = tokenizer.decode([nxt], skip_special_tokens=True).replace(\"\\n\",\"\\\\n\")\n",
    "            generated_text += token_str\n",
    "            out_ids = torch.cat([out_ids, torch.tensor([nxt], device=DEVICE)])\n",
    "        \n",
    "        # Print trace line if in debug mode\n",
    "        if debug:\n",
    "            print(f\"{gen_len:4d} | {int(steering)} | {p_red:5.3f} | {err:5.3f} | \"\n",
    "                  f\"{alpha:5.3f} | {I:5.3f} | {D:5.3f} | {temp:5.3f} | '{token_str}'\")\n",
    "        \n",
    "        # Check stop condition\n",
    "        if stop_re.search(generated_text) or \"Final answer:\" in generated_text:\n",
    "            if debug:\n",
    "                print(f\"[INFO] Stop condition met, ending generation\")\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    if debug:\n",
    "        print(\"--- END TRACE ---\\n\")\n",
    "        print(f\"=== GENERATION RESULT ===\")\n",
    "        print(f\"Total tokens generated: {out_ids.size(0) - ids.size(0)}\")\n",
    "        print(f\"Final text: {tokenizer.decode(out_ids, skip_special_tokens=True)}\")\n",
    "    \n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True), out_ids.size(0) - ids.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef07dca7-59dd-44f7-9873-6005cbf5f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_answer(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the content of the last \\boxed{} occurrence in the string.\n",
    "    This handles cases where the model might have multiple boxed answers,\n",
    "    ensuring we get the final one.\n",
    "    \n",
    "    Args:\n",
    "        s: The string to extract the answer from\n",
    "        \n",
    "    Returns:\n",
    "        The content inside the last \\boxed{} occurrence, or empty string if none found\n",
    "    \"\"\"\n",
    "    # Find all occurrences of \\boxed{...}\n",
    "    matches = list(re.finditer(r\"\\\\boxed\\{([^}]+)\\}\", s))\n",
    "    \n",
    "    # Return the last match, if any\n",
    "    if matches:\n",
    "        return matches[-1].group(1).strip()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9571b1-b8ee-4198-b775-5072f1a066aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gsm8k(n_probs, max_tokens, debug = False):\n",
    "    gsm = load_dataset(\"gsm8k\", \"main\")[\"test\"].select(range(1000,1000+n_probs))\n",
    "    rec = []\n",
    "    baseline_total = 0\n",
    "    raspid_total = 0\n",
    "    for ex in tqdm(gsm):\n",
    "        q   = ex[\"question\"].strip()\n",
    "        prompt = f\"{q}\\n\\nAnswer step by step and end with: Final answer: \\\\boxed{{numeric_value}}\"\n",
    "        r_txt,r_tok = generate_raspid(prompt, max_tokens, debug = debug)\n",
    "        b_txt,b_tok = generate_baseline(prompt, max_tokens)\n",
    "        \n",
    "\n",
    "        rec.append({\n",
    "            \"reference_answer\":ex[\"answer\"],\n",
    "            \"baseline_correct\": norm_answer(b_txt),\n",
    "            \"raspid_correct\":  norm_answer(r_txt),\n",
    "            \"baseline_tokens\": b_tok,\n",
    "            \"raspid_tokens\":  r_tok,\n",
    "            \"baseline_txt\": b_txt,\n",
    "            \"raspid_txt\":  r_txt,\n",
    "\n",
    "        })\n",
    "        baseline_total += int(b_tok)\n",
    "        raspid_total += int(r_tok)\n",
    "        print(f'total-token-usage for baseline: {baseline_total} raspid: {raspid_total}')\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25f36f7-3aa6-45f1-90a8-94fdb3723ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "      <th>solution</th>\n",
       "      <th>baseline_generation</th>\n",
       "      <th>all_pred_baseline</th>\n",
       "      <th>all_eval_baseline</th>\n",
       "      <th>mv_pred_baseline</th>\n",
       "      <th>mv_eval_baseline</th>\n",
       "      <th>mv_index_baseline</th>\n",
       "      <th>seal_generation</th>\n",
       "      <th>all_pred_seal</th>\n",
       "      <th>all_eval_seal</th>\n",
       "      <th>mv_pred_seal</th>\n",
       "      <th>mv_eval_seal</th>\n",
       "      <th>mv_index_seal</th>\n",
       "      <th>baseline_tokens</th>\n",
       "      <th>seal_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;｜User｜&gt;Answer the following questions. You sh...</td>\n",
       "      <td>Convert the point $(0,3)$ in rectangular coord...</td>\n",
       "      <td>\\left( 3, \\frac{\\pi}{2} \\right)</td>\n",
       "      <td>We have that $r = \\sqrt{0^2 + 3^2} = 3.$  Also...</td>\n",
       "      <td>[\"Okay, so I need to convert the rectangular c...</td>\n",
       "      <td>['(3,\\\\frac{\\\\pi}{2})']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>(3,\\frac{\\pi}{2})</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Okay, so I need to convert the rectangular c...</td>\n",
       "      <td>['(3,\\\\frac{\\\\pi}{2})']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>(3,\\frac{\\pi}{2})</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>936</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;｜User｜&gt;Answer the following questions. You sh...</td>\n",
       "      <td>Define\\n\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2...</td>\n",
       "      <td>p - q</td>\n",
       "      <td>We count the number of times $\\frac{1}{n^3}$ a...</td>\n",
       "      <td>[\"Okay, so I have this problem where I need to...</td>\n",
       "      <td>['2']</td>\n",
       "      <td>[False]</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Okay, so I have this problem where I need to...</td>\n",
       "      <td>['p-q']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>p-q</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>11321</td>\n",
       "      <td>10773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;｜User｜&gt;Answer the following questions. You sh...</td>\n",
       "      <td>If $f(x) = \\frac{3x-2}{x-2}$, what is the valu...</td>\n",
       "      <td>\\frac{14}{3}</td>\n",
       "      <td>$f(-2)+f(-1)+f(0)=\\frac{3(-2)-2}{-2-2}+\\frac{3...</td>\n",
       "      <td>[\"Okay, so I have this function f(x) = (3x - 2...</td>\n",
       "      <td>['\\\\frac{14}{3}']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>\\frac{14}{3}</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Okay, so I have this function f(x) = (3x - 2...</td>\n",
       "      <td>['\\\\frac{14}{3}']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>\\frac{14}{3}</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1525</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;｜User｜&gt;Answer the following questions. You sh...</td>\n",
       "      <td>How many positive whole-number divisors does 1...</td>\n",
       "      <td>9</td>\n",
       "      <td>First prime factorize $196=2^2\\cdot7^2$.  The ...</td>\n",
       "      <td>[\"Okay, so I need to figure out how many posit...</td>\n",
       "      <td>['9']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Okay, so I need to figure out how many posit...</td>\n",
       "      <td>['9']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1497</td>\n",
       "      <td>1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;｜User｜&gt;Answer the following questions. You sh...</td>\n",
       "      <td>The results of a cross-country team's training...</td>\n",
       "      <td>\\text{Evelyn}</td>\n",
       "      <td>Evelyn covered more distance in less time than...</td>\n",
       "      <td>[\"Okay, so I have this cross-country team's tr...</td>\n",
       "      <td>['Evelyn']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>Evelyn</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Okay, so I have this cross-country team's tr...</td>\n",
       "      <td>['Evelyn']</td>\n",
       "      <td>[True]</td>\n",
       "      <td>Evelyn</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2434</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  <｜User｜>Answer the following questions. You sh...   \n",
       "1  <｜User｜>Answer the following questions. You sh...   \n",
       "2  <｜User｜>Answer the following questions. You sh...   \n",
       "3  <｜User｜>Answer the following questions. You sh...   \n",
       "4  <｜User｜>Answer the following questions. You sh...   \n",
       "\n",
       "                                             problem  \\\n",
       "0  Convert the point $(0,3)$ in rectangular coord...   \n",
       "1  Define\\n\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2...   \n",
       "2  If $f(x) = \\frac{3x-2}{x-2}$, what is the valu...   \n",
       "3  How many positive whole-number divisors does 1...   \n",
       "4  The results of a cross-country team's training...   \n",
       "\n",
       "                            answer  \\\n",
       "0  \\left( 3, \\frac{\\pi}{2} \\right)   \n",
       "1                            p - q   \n",
       "2                     \\frac{14}{3}   \n",
       "3                                9   \n",
       "4                    \\text{Evelyn}   \n",
       "\n",
       "                                            solution  \\\n",
       "0  We have that $r = \\sqrt{0^2 + 3^2} = 3.$  Also...   \n",
       "1  We count the number of times $\\frac{1}{n^3}$ a...   \n",
       "2  $f(-2)+f(-1)+f(0)=\\frac{3(-2)-2}{-2-2}+\\frac{3...   \n",
       "3  First prime factorize $196=2^2\\cdot7^2$.  The ...   \n",
       "4  Evelyn covered more distance in less time than...   \n",
       "\n",
       "                                 baseline_generation        all_pred_baseline  \\\n",
       "0  [\"Okay, so I need to convert the rectangular c...  ['(3,\\\\frac{\\\\pi}{2})']   \n",
       "1  [\"Okay, so I have this problem where I need to...                    ['2']   \n",
       "2  [\"Okay, so I have this function f(x) = (3x - 2...        ['\\\\frac{14}{3}']   \n",
       "3  [\"Okay, so I need to figure out how many posit...                    ['9']   \n",
       "4  [\"Okay, so I have this cross-country team's tr...               ['Evelyn']   \n",
       "\n",
       "  all_eval_baseline   mv_pred_baseline  mv_eval_baseline  mv_index_baseline  \\\n",
       "0            [True]  (3,\\frac{\\pi}{2})              True                  0   \n",
       "1           [False]                  2             False                  0   \n",
       "2            [True]       \\frac{14}{3}              True                  0   \n",
       "3            [True]                  9              True                  0   \n",
       "4            [True]             Evelyn              True                  0   \n",
       "\n",
       "                                     seal_generation            all_pred_seal  \\\n",
       "0  [\"Okay, so I need to convert the rectangular c...  ['(3,\\\\frac{\\\\pi}{2})']   \n",
       "1  [\"Okay, so I have this problem where I need to...                  ['p-q']   \n",
       "2  [\"Okay, so I have this function f(x) = (3x - 2...        ['\\\\frac{14}{3}']   \n",
       "3  [\"Okay, so I need to figure out how many posit...                    ['9']   \n",
       "4  [\"Okay, so I have this cross-country team's tr...               ['Evelyn']   \n",
       "\n",
       "  all_eval_seal       mv_pred_seal  mv_eval_seal  mv_index_seal  \\\n",
       "0        [True]  (3,\\frac{\\pi}{2})          True              0   \n",
       "1        [True]                p-q          True              0   \n",
       "2        [True]       \\frac{14}{3}          True              0   \n",
       "3        [True]                  9          True              0   \n",
       "4        [True]             Evelyn          True              0   \n",
       "\n",
       "   baseline_tokens  seal_tokens  \n",
       "0              936          881  \n",
       "1            11321        10773  \n",
       "2             1525         1593  \n",
       "3             1497         1487  \n",
       "4             2434         2260  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seal_results = pd.read_csv('seal_results.csv')    \n",
    "seal_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa96494-5519-4061-9985-a12508ed5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "seal_df = pd.read_csv('seal_results.csv')\n",
    "\n",
    "# baseline_tokens = []\n",
    "# seal_tokens = []\n",
    "\n",
    "# for i in tqdm(range(len(seal_df)), desc=\"Counting tokens\"):\n",
    "#     baseline_tokens.append(len(tokenizer(seal_df.iloc[i]['baseline_generation'])['input_ids']))\n",
    "#     seal_tokens.append(len(tokenizer(seal_df.iloc[i]['seal_generation'])['input_ids']))\n",
    "\n",
    "# seal_df['baseline_tokens'] = baseline_tokens\n",
    "# seal_df['seal_tokens'] = seal_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c81267e-a236-4bca-8ef6-20bad49e75e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2743.0, 1923.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seal_df.to_csv('seal_results.csv')\n",
    "seal_df['baseline_tokens'].median(),seal_df['seal_tokens'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e64781-ff06-4610-918d-6abcfc705786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seal(n_probs, max_tokens, debug = False):\n",
    "    \n",
    "    seal_results = seal_df.iloc[:n_probs]\n",
    "\n",
    "    extract_prompt = lambda q: q.split('<｜User｜>')[1].split('<｜Assistant｜>')[0].strip()\n",
    "    \n",
    "    rec = []\n",
    "    baseline_total = 0\n",
    "    raspid_total = 0\n",
    "    \n",
    "    for idx,ex in tqdm(seal_results.iterrows()):\n",
    "        \n",
    "        prompt = extract_prompt(ex[\"prompt\"])\n",
    "        ref_answer = ex[\"answer\"]\n",
    "        \n",
    "        baseline_txt = str(eval(ex['baseline_generation'])[0])\n",
    "        seal_txt = str(eval(ex['seal_generation'])[0])\n",
    "        raspid_txt,raspid_tokens = generate_raspid(prompt, max_tokens, debug = debug)\n",
    "\n",
    "        baseline_ans = eval(ex['all_pred_baseline'])[0]\n",
    "        seal_ans = eval(ex['all_pred_seal'])[0]\n",
    "        raspid_ans = norm_answer(raspid_txt)\n",
    "\n",
    "\n",
    "        baseline_tokens = ex['baseline_tokens']\n",
    "        seal_tokens = ex['seal_tokens']\n",
    "\n",
    "\n",
    "        rec.append({\n",
    "            \"prompt\":prompt,\n",
    "            \"reference_answer\":ref_answer,\n",
    "\n",
    "                        \n",
    "            \"baseline_txt\": baseline_txt,\n",
    "            \"seal_txt\":  seal_txt,\n",
    "            \"raspid_txt\":  raspid_txt,\n",
    "\n",
    "            \n",
    "            \"baseline_answer\": baseline_ans,\n",
    "            \"seal_answer\":  seal_ans,\n",
    "            \"raspid_answer\":  raspid_ans,\n",
    "\n",
    "            \n",
    "            \"baseline_tokens\": baseline_tokens,\n",
    "            \"seal_tokens\": seal_tokens,\n",
    "            \"raspid_tokens\":  raspid_tokens,\n",
    "\n",
    "        })\n",
    "        print(f\"baseline_tokens: {baseline_tokens}, seal_tokens: {seal_tokens}, raspid_tokens: {raspid_tokens}\")\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87b7cf5c-9f4d-498a-a8c9-5b7446b3865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = run_seal(5, 4096*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d333258-3570-4b0a-90d9-28730877042d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196704b74c4543769b4112f52fcefbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f115d198ec584457a9f267c5e52a1230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 2801 raspid: 563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf1b09bb7494f17878a08939abd714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 5444 raspid: 4659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82a601276c947a59765447bb07643c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 7309 raspid: 5278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca67b0e71a384d1f9877b0dd16c1ccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 10663 raspid: 5594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce08eb55433340978c653a2c5ca71221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 12220 raspid: 5992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3712678cebc84784953827f2b6026e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 13980 raspid: 7103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be6b28a19354122b79825ef0fcd9821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 15855 raspid: 8557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a83cb5541fa410fbc933d28fdcecfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 18011 raspid: 8895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3436da738ed54ee38bfb6ba7654eea63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 18636 raspid: 9277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea2319d179d471f84d848124c72da8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 20831 raspid: 10154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4290dc6aae9c4683a155c67364c06cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 22644 raspid: 10675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c36184ebb67475d89cfd80cbf2c584b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 25130 raspid: 12408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3903d4e5948a408381e08ca8149e2858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 29226 raspid: 16504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776a43690ea745deac450a58cadb75ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 33322 raspid: 17359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d109a958340242efaea40e349a3b7a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 35763 raspid: 21455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccdcd462c484bd4ae5e388fdb32286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 37196 raspid: 21878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a31155c9cdf4230a706a6518824206a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 39617 raspid: 23983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390a1bf0a17b43839a743b55ed87daa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 43713 raspid: 25086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfc3149fd244af79075a43f45fe25fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 45873 raspid: 25999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6e6f7c12ad41fe9e151ea5e22553b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 46960 raspid: 26603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe8ca9158864847848e1726713d05b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 47288 raspid: 26821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744b1997dd444c2b88aa7570f6713034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 48631 raspid: 27543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e732a0acd534e67958f67ccad7df634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 49766 raspid: 28249\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acacfee19db473fa052d5995ae88052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 50704 raspid: 29046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4fe4b58ac04a9aa7c023039b464a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 52657 raspid: 29450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaef3b494624ed8a78f25fbec38a0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 53023 raspid: 30417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4649b8092141410d8f56a8a66b65bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 54017 raspid: 30980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b14bc9163bd40c39efe38a2016c7e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 55717 raspid: 31414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda6aeaa636d4def9606a8d02204b3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 56157 raspid: 31675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b21c49baad4181844df9bf27c5b373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 56880 raspid: 32213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b879f4c08ff94595ad360d28798b7d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 60976 raspid: 32983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2408175ccc274797bd7518dfaab18ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 62737 raspid: 33687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1dedefd8e745ee8034b8800cacd935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 66833 raspid: 34136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4338d084679d4b6383857255eb8501b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 67581 raspid: 34728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8aa0b92c264ebfa4cf26c3430c1236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 67902 raspid: 35157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65c846619174a7580d4cdc3f621ac7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 69792 raspid: 39253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46f2add39274bd0bb767daa5a69361f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 70036 raspid: 39559\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecc31ec3e484d59916d0e63b4d12efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 71200 raspid: 40143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc6eda14c444069df2179d2a072592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 75296 raspid: 41920\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0fc6a6726f416f92664c4739e3a0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 75826 raspid: 44638\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb976e082d742a090cb39616ae2fba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 76192 raspid: 45116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145081e5c8d74e0f957aba2849cc9bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 77148 raspid: 45788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64ad1fd8f5e4eeeb82ab4b3aaf44b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 77670 raspid: 47702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533240561a714d38a73546320d80bd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 79156 raspid: 48724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6c79a65f7a4c0a88079f278e9bd849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 80303 raspid: 50154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fc62fabc1144b9be2aa9de9e1a236a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 80517 raspid: 51118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0619426e3b23452eb68d25ad00e54210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 82113 raspid: 51570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651689e2b8554b3d97e59e79769b102d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 82739 raspid: 55666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a96c7fd9ff45878920bd2cfe8a6f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 86835 raspid: 58081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8765cfc9f941fb9760d2aa1eabe7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RASPID gen:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total-token-usage for baseline: 87218 raspid: 59142\n"
     ]
    }
   ],
   "source": [
    "results_df = run_gsm8k(50, 4096)\n",
    "results_df.to_csv('results_df_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83cb7920-7c47-4259-9d6a-91c0e5c85e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_subset = load_dataset(\"gsm8k\", \"main\")[\"test\"].select(range(1000,1000+50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97ab03a2-5737-4602-b5b0-71bb97e6f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_answers = [gsm_subset['answer'][idx].split('#### ')[1] for idx in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c6844fb-4879-42d0-86ce-ce75ec12a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['reference_correct'] = reference_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95f731d5-f533-41a5-bddc-3d074d48c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = len(results_df[results_df['baseline_correct'] == results_df['reference_correct']])/len(results_df)\n",
    "raspid_acc = len(results_df[results_df['raspid_correct'] == results_df['reference_correct']])/len(results_df)\n",
    "token_eff = (results_df['baseline_tokens'].mean() - results_df['raspid_tokens'].mean())/results_df['baseline_tokens'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3310f923-bdec-4005-9855-1c650f87975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline-acc: 0.58 raspid-acc: 0.64 token-saving: 0.3219060285720838\n"
     ]
    }
   ],
   "source": [
    "print(f'baseline-acc: {baseline_acc} raspid-acc: {raspid_acc} token-saving: {token_eff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c12bc143-12ed-4c8f-b7a4-cabfa99a8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## raspid works better than rasped fixed v2 in 66% of the cases with 52% token saving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
