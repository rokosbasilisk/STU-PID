{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd63ff03-3a90-4d0e-9797-8a761e9e601a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install repeng accelerate datasets matplotlib seaborn vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b393130-edc2-4c7b-ba32-e38051fbcfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "RASPID (Reasoning-Aware Steering with PID control):\n",
    "A method for dynamic control of language model generation using chunk-level classification\n",
    "and PID-based steering to optimize reasoning quality while reducing redundancy.\n",
    "\n",
    "This implementation:\n",
    "- Trains a chunk-level classifier on labeled reasoning chains\n",
    "- Builds control vectors from required vs redundant thoughts\n",
    "- Uses PID control to dynamically steer generation away from redundant patterns\n",
    "- Evaluates performance on GSM8K mathematical reasoning tasks\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from repeng import ControlModel, ControlVector\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", \"To copy construct from a tensor\", UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "class RASPIDConfig:\n",
    "    \"\"\"Configuration class for RASPID hyperparameters\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    DTYPE = torch.float32\n",
    "    \n",
    "    # Data paths\n",
    "    LABELED_CSV = \"results/gsm8k_chains_labeled_with_tokens.csv\"\n",
    "    CTRL_VEC_PATH = \"ctrl_vector.pt\"\n",
    "    \n",
    "    # Classifier hyperparameters\n",
    "    EMB_LAYER = 20\n",
    "    CHUNK_SIZES = [16, 24]\n",
    "    BATCH_SIZE = 32\n",
    "    FLUFF_STAR = 0.5  # Target probability for \"redundant\"\n",
    "    \n",
    "    # PID steering hyperparameters\n",
    "    INIT_FREE = 80      # Let model reason freely first\n",
    "    STEER_WINDOW = 60   # Duration of steering intervention\n",
    "    KP, KI, KD = 0.05, 0.001, 0.001  # PID coefficients\n",
    "    MAX_I = 0.20        # Maximum integral term\n",
    "    MAX_ALPHA = 0.40    # Maximum steering coefficient\n",
    "    STEER_MARGIN = 0.20 # Margin above FLUFF_STAR to trigger steering\n",
    "    \n",
    "    # Generation parameters\n",
    "    BASE_TEMP = 0.60    # Base temperature for sampling\n",
    "    STEER_TEMP = 0.30   # Temperature during steering\n",
    "    MAX_REPEAT = 8      # Maximum consecutive token repetitions\n",
    "    MAX_TOKENS = 4096*8   # Maximum tokens to generate\n",
    "    MAX_RAW = 50.0      # Clamp value for classifier raw scores\n",
    "    \n",
    "    # Performance optimizations\n",
    "    ENABLE_COMPILE = True  # Whether to use torch.compile for speedup\n",
    "    MEMORY_EFFICIENT = True  # Whether to use memory-efficient attention\n",
    "    PROGRESS_UPDATE_FREQ = 10  # How often to update progress (every N problems)\n",
    "\n",
    "\n",
    "class ChunkDataset(Dataset):\n",
    "    \"\"\"Dataset for creating fixed-size chunks from text sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, label, chunk_size, tokenizer):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunks = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tok_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "            # Create overlapping chunks\n",
    "            for i in range(0, len(tok_ids) - chunk_size + 1, chunk_size):\n",
    "                chunk = tok_ids[i:i + chunk_size]\n",
    "                self.chunks.append(chunk)\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunks[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class ChunkClassifier:\n",
    "    \"\"\"Chunk-level classifier for identifying redundant vs required reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, config, tokenizer, model):\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.best_classifier = None\n",
    "        self.best_chunk_size = None\n",
    "        self.best_accuracy = 0.0\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Collate function for DataLoader\"\"\"\n",
    "        input_ids, labels = zip(*batch)\n",
    "        # Pad sequences on CPU for pin_memory efficiency\n",
    "        seqs = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(\n",
    "            seqs, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        attention_mask = (padded != self.tokenizer.pad_token_id).long()\n",
    "        return {\"input_ids\": padded, \"attention_mask\": attention_mask}, torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def _extract_embeddings(self, loader):\n",
    "        \"\"\"Extract embeddings from model for all chunks\"\"\"\n",
    "        features, labels = [], []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_tokens, batch_labels in tqdm(loader, desc=\"Extracting embeddings\"):\n",
    "                # Move batch to device\n",
    "                batch_tokens = {\n",
    "                    k: v.to(self.config.DEVICE, non_blocking=True)\n",
    "                    for k, v in batch_tokens.items()\n",
    "                }\n",
    "                \n",
    "                # Get model outputs and extract embeddings\n",
    "                outputs = self.model(**batch_tokens, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states[self.config.EMB_LAYER]\n",
    "                embeddings = hidden_states.mean(dim=1)  # Average pooling\n",
    "                \n",
    "                features.append(embeddings.cpu().numpy())\n",
    "                labels.append(batch_labels.numpy())\n",
    "        \n",
    "        return np.vstack(features), np.concatenate(labels)\n",
    "    \n",
    "    def _train_sgd_classifier(self, X_train, y_train, X_val, y_val, chunk_size):\n",
    "        \"\"\"Train SGD classifier with early stopping\"\"\"\n",
    "        classifier = SGDClassifier(\n",
    "            loss=\"log_loss\", random_state=42, warm_start=True, max_iter=1, tol=None\n",
    "        )\n",
    "        \n",
    "        prev_coef = None\n",
    "        pbar = tqdm(range(500), desc=f\"Training classifier (cs={chunk_size})\", leave=False)\n",
    "        \n",
    "        for iteration in pbar:\n",
    "            classifier.fit(X_train, y_train)\n",
    "            current_coef = classifier.coef_\n",
    "            \n",
    "            if prev_coef is not None:\n",
    "                delta = np.max(np.abs(current_coef - prev_coef))\n",
    "                pbar.set_postfix(delta=delta)\n",
    "                if delta < 1e-3:  # Convergence criterion\n",
    "                    break\n",
    "            prev_coef = current_coef.copy()\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_accuracy = accuracy_score(y_val, classifier.predict(X_val))\n",
    "        return classifier, val_accuracy\n",
    "    \n",
    "    def train(self, required_texts, redundant_texts):\n",
    "        \"\"\"Train classifier on required vs redundant text chunks\"\"\"\n",
    "        print(\"Training chunk classifier...\")\n",
    "        \n",
    "        for chunk_size in self.config.CHUNK_SIZES:\n",
    "            print(f\"Testing chunk size: {chunk_size}\")\n",
    "            \n",
    "            # Create datasets\n",
    "            dataset_required = ChunkDataset(required_texts, 0, chunk_size, self.tokenizer)\n",
    "            dataset_redundant = ChunkDataset(redundant_texts, 1, chunk_size, self.tokenizer)\n",
    "            \n",
    "            # Create data loader\n",
    "            combined_dataset = ConcatDataset([dataset_required, dataset_redundant])\n",
    "            loader = DataLoader(\n",
    "                combined_dataset,\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                collate_fn=self._collate_fn,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "            \n",
    "            # Extract embeddings\n",
    "            X, y = self._extract_embeddings(loader)\n",
    "            \n",
    "            # Train/validation split\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Train classifier\n",
    "            classifier, accuracy = self._train_sgd_classifier(\n",
    "                X_train, y_train, X_val, y_val, chunk_size\n",
    "            )\n",
    "            \n",
    "            print(f\"Chunk size {chunk_size} → Validation accuracy: {accuracy:.3f}\")\n",
    "            \n",
    "            # Keep track of best performer\n",
    "            if accuracy > self.best_accuracy:\n",
    "                self.best_chunk_size = chunk_size\n",
    "                self.best_accuracy = accuracy\n",
    "                self.best_classifier = classifier\n",
    "        \n",
    "        print(f\"✅ Best classifier: chunk_size={self.best_chunk_size}, accuracy={self.best_accuracy:.3f}\")\n",
    "        return self.best_classifier, self.best_chunk_size\n",
    "\n",
    "\n",
    "class ControlVectorBuilder:\n",
    "    \"\"\"Builder for control vectors from text contrasts\"\"\"\n",
    "    \n",
    "    def __init__(self, config, tokenizer, model):\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "    \n",
    "    def _compute_mean_hidden_state(self, texts):\n",
    "        \"\"\"Compute mean hidden state across texts\"\"\"\n",
    "        hidden_states = []\n",
    "        \n",
    "        for text in tqdm(texts, desc=\"Computing hidden states\"):\n",
    "            tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True).to(self.config.DEVICE)\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model(**tokens, output_hidden_states=True)\n",
    "                hidden_state = outputs.hidden_states[self.config.EMB_LAYER][0]\n",
    "                mean_hidden = hidden_state.mean(dim=0)\n",
    "                hidden_states.append(mean_hidden.cpu())\n",
    "        \n",
    "        return torch.stack(hidden_states).mean(dim=0)\n",
    "    \n",
    "    def build_control_vector(self, required_texts, redundant_texts):\n",
    "        \"\"\"Build control vector from required vs redundant text contrasts\"\"\"\n",
    "        print(\"Building control vector...\")\n",
    "        \n",
    "        # Compute mean hidden states\n",
    "        v_required = self._compute_mean_hidden_state(required_texts)\n",
    "        v_redundant = self._compute_mean_hidden_state(redundant_texts)\n",
    "        \n",
    "        # Create control vector\n",
    "        model_type = self.model.config.model_type\n",
    "        control_vector = ControlVector(\n",
    "            model_type=model_type,\n",
    "            directions={self.config.EMB_LAYER: (v_required - v_redundant).to(self.config.DEVICE)}\n",
    "        )\n",
    "        \n",
    "        # Save control vector\n",
    "        torch.save(control_vector, self.config.CTRL_VEC_PATH)\n",
    "        print(\"✅ Control vector saved\")\n",
    "        \n",
    "        return control_vector\n",
    "\n",
    "\n",
    "class RASPIDGenerator:\n",
    "    \"\"\"RASPID generator with PID-controlled steering\"\"\"\n",
    "    \n",
    "    def __init__(self, config, tokenizer, base_model, control_model, classifier, chunk_size, control_vector):\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.base_model = base_model\n",
    "        self.control_model = control_model\n",
    "        self.classifier = classifier\n",
    "        self.chunk_size = chunk_size\n",
    "        self.control_vector = control_vector\n",
    "        self.stop_pattern = re.compile(r\"\\\\boxed\\{[^{}]{1,12}\\}\")\n",
    "        \n",
    "        # Pre-compile regex patterns for efficiency\n",
    "        self.final_answer_pattern = re.compile(r\"Final answer:\", re.IGNORECASE)\n",
    "        \n",
    "        # Pre-compute some constants to avoid repeated calculations\n",
    "        self.max_alpha_inv = 1.0 / self.config.MAX_ALPHA\n",
    "        self.temp_diff = self.config.BASE_TEMP - self.config.STEER_TEMP\n",
    "    \n",
    "    def _handle_numerical_stability(self, tensor, name=\"tensor\"):\n",
    "        \"\"\"Handle NaN values and extreme values in tensors\"\"\"\n",
    "        if torch.isnan(tensor).any():\n",
    "            return torch.nan_to_num(tensor, nan=0.0, posinf=100.0, neginf=-100.0)\n",
    "        return tensor\n",
    "    \n",
    "    def _scale_extreme_classifier_output(self, raw_score):\n",
    "        \"\"\"Scale down extreme classifier outputs for stability - optimized\"\"\"\n",
    "        abs_raw = abs(raw_score)\n",
    "        if abs_raw > self.config.MAX_RAW:\n",
    "            return self.config.MAX_RAW * np.sign(raw_score) * np.tanh(abs_raw / self.config.MAX_RAW)\n",
    "        return np.clip(raw_score, -self.config.MAX_RAW, self.config.MAX_RAW)\n",
    "    \n",
    "    def _should_apply_steering(self, generation_length, steering_active, steering_start):\n",
    "        \"\"\"Determine if steering should be applied - optimized\"\"\"\n",
    "        if not steering_active:\n",
    "            return generation_length >= self.config.INIT_FREE, generation_length\n",
    "        elif generation_length - steering_start > self.config.STEER_WINDOW:\n",
    "            return False, steering_start\n",
    "        return True, steering_start\n",
    "    \n",
    "    def _update_pid_controller(self, p_redundant, alpha, integral, derivative, prev_error):\n",
    "        \"\"\"Update PID controller based on redundancy probability - optimized\"\"\"\n",
    "        threshold = self.config.FLUFF_STAR + self.config.STEER_MARGIN\n",
    "        if p_redundant <= threshold:\n",
    "            return alpha, integral, derivative, 0.0\n",
    "        \n",
    "        error = p_redundant - self.config.FLUFF_STAR\n",
    "        \n",
    "        # Update PID terms with clamping\n",
    "        integral = np.clip(integral + self.config.KI * error, -self.config.MAX_I, self.config.MAX_I)\n",
    "        derivative = self.config.KD * (error - prev_error) + (1 - self.config.KD) * derivative\n",
    "        \n",
    "        # Calculate new alpha with clamping\n",
    "        pid_output = self.config.KP * error + integral + derivative\n",
    "        alpha = np.clip(alpha + pid_output, 0.0, self.config.MAX_ALPHA)\n",
    "        \n",
    "        return alpha, integral, derivative, error\n",
    "    \n",
    "    def _fast_temperature_calc(self, coefficient):\n",
    "        \"\"\"Fast temperature calculation using pre-computed constants\"\"\"\n",
    "        ratio = coefficient * self.max_alpha_inv\n",
    "        return self.config.BASE_TEMP - self.temp_diff * ratio\n",
    "    \n",
    "    def _early_stop_check(self, generated_text):\n",
    "        \"\"\"Check if generation should stop early - optimized\"\"\"\n",
    "        return (self.stop_pattern.search(generated_text) is not None or \n",
    "                self.final_answer_pattern.search(generated_text) is not None)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt, max_new_tokens=None, debug=False):\n",
    "        \"\"\"Generate text using RASPID with PID-controlled steering - optimized\"\"\"\n",
    "        if max_new_tokens is None:\n",
    "            max_new_tokens = self.config.MAX_TOKENS\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\n=== RASPID GENERATION ===\")\n",
    "            print(f\"Prompt: '{prompt}'\")\n",
    "            print(f\"Max tokens: {max_new_tokens}\")\n",
    "            print(\"--- GENERATION TRACE ---\")\n",
    "            print(\"step | steering | p_red |  err  |   α   |   I   |   D   | temp | token\")\n",
    "        \n",
    "        # Initialize generation state\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.config.DEVICE).input_ids[0]\n",
    "        output_ids = input_ids.clone()\n",
    "        past_key_values = None\n",
    "        \n",
    "        # Initialize PID controller state\n",
    "        alpha = integral = derivative = prev_error = 0.0\n",
    "        chunk_hidden = None\n",
    "        tokens_in_chunk = 0\n",
    "        steering_active = False\n",
    "        steering_start = 0\n",
    "        \n",
    "        # Initialize repetition detection\n",
    "        last_token = None\n",
    "        repetition_count = 0\n",
    "        generated_text = \"\"\n",
    "        \n",
    "        # Pre-allocate arrays for batch operations where possible\n",
    "        device = self.config.DEVICE\n",
    "        \n",
    "        # Generation loop\n",
    "        if not debug:\n",
    "            pbar = tqdm(range(max_new_tokens), desc=\"RASPID generation\", leave=False)\n",
    "        else:\n",
    "            pbar = range(max_new_tokens)\n",
    "        \n",
    "        for step in pbar:\n",
    "            generation_length = output_ids.size(0) - input_ids.size(0)\n",
    "            \n",
    "            # Update steering state\n",
    "            steering_active, steering_start = self._should_apply_steering(\n",
    "                generation_length, steering_active, steering_start\n",
    "            )\n",
    "            \n",
    "            # Apply control vector\n",
    "            coefficient = alpha if steering_active else 0.0\n",
    "            self.control_model.set_control(self.control_vector, coeff=coefficient)\n",
    "            \n",
    "            # Forward pass - optimized\n",
    "            if generation_length == 0:\n",
    "                # First step: process entire prompt\n",
    "                outputs = self.control_model(\n",
    "                    input_ids=output_ids.unsqueeze(0),\n",
    "                    use_cache=True,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            else:\n",
    "                # Subsequent steps: process only new token\n",
    "                outputs = self.control_model(\n",
    "                    input_ids=output_ids[-1:].unsqueeze(0),\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            \n",
    "            past_key_values = outputs.past_key_values\n",
    "            logits = outputs.logits[0, -1]\n",
    "            hidden_state = outputs.hidden_states[self.config.EMB_LAYER][0, -1]\n",
    "            \n",
    "            # Handle numerical stability - optimized\n",
    "            logits = self._handle_numerical_stability(logits)\n",
    "            hidden_state = self._handle_numerical_stability(hidden_state)\n",
    "            \n",
    "            # Check for token repetition\n",
    "            current_token = output_ids[-1].item()\n",
    "            if current_token == last_token:\n",
    "                repetition_count += 1\n",
    "                if repetition_count >= self.config.MAX_REPEAT:\n",
    "                    if debug:\n",
    "                        print(f\"[INFO] Maximum repetition reached, stopping generation\")\n",
    "                    break\n",
    "            else:\n",
    "                repetition_count = 0\n",
    "                last_token = current_token\n",
    "            \n",
    "            # Update chunk tracking - optimized\n",
    "            hidden_norm = torch.norm(hidden_state)\n",
    "            if hidden_norm > 1e-8:\n",
    "                hidden_normalized = hidden_state / hidden_norm\n",
    "            else:\n",
    "                hidden_normalized = hidden_state\n",
    "            \n",
    "            chunk_hidden = hidden_normalized if chunk_hidden is None else chunk_hidden + hidden_normalized\n",
    "            tokens_in_chunk += 1\n",
    "            \n",
    "            # Classifier prediction and PID update - optimized\n",
    "            p_redundant = error = 0.0\n",
    "            if tokens_in_chunk >= self.chunk_size:\n",
    "                try:\n",
    "                    # Prepare classifier input - optimized\n",
    "                    classifier_input = (chunk_hidden / self.chunk_size).cpu().numpy().reshape(1, -1)\n",
    "                    \n",
    "                    # Handle potential NaN values\n",
    "                    if np.isnan(classifier_input).any():\n",
    "                        classifier_input = np.nan_to_num(classifier_input)\n",
    "                    \n",
    "                    # Get classifier prediction - single call\n",
    "                    raw_score = self.classifier.decision_function(classifier_input)[0]\n",
    "                    raw_score = self._scale_extreme_classifier_output(raw_score)\n",
    "                    \n",
    "                    # Fast sigmoid approximation for better performance\n",
    "                    if abs(raw_score) < 5.0:  # Use exact sigmoid for reasonable values\n",
    "                        p_redundant = 1.0 / (1.0 + math.exp(-raw_score))\n",
    "                    else:  # Use approximation for extreme values\n",
    "                        p_redundant = 1.0 if raw_score > 0 else 0.0\n",
    "                    \n",
    "                    # Update PID controller\n",
    "                    alpha, integral, derivative, error = self._update_pid_controller(\n",
    "                        p_redundant, alpha, integral, derivative, prev_error\n",
    "                    )\n",
    "                    prev_error = error\n",
    "                    \n",
    "                    # Reset chunk\n",
    "                    chunk_hidden = None\n",
    "                    tokens_in_chunk = 0\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if debug:\n",
    "                        print(f\"[ERROR] Classifier/PID error: {e}\")\n",
    "                    chunk_hidden = hidden_normalized\n",
    "                    tokens_in_chunk = 1\n",
    "            \n",
    "            # Temperature adjustment and sampling - optimized\n",
    "            temperature = self._fast_temperature_calc(coefficient)\n",
    "            \n",
    "            try:\n",
    "                # Safe sampling with optimized operations\n",
    "                logits_scaled = logits / temperature\n",
    "                logits_clamped = torch.clamp(logits_scaled, -50, 50)  # Smaller range for speed\n",
    "                probabilities = torch.softmax(logits_clamped, dim=-1)\n",
    "                \n",
    "                # Sample token\n",
    "                next_token = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[ERROR] Sampling error: {e}, using argmax\")\n",
    "                next_token = torch.argmax(logits).item()\n",
    "            \n",
    "            # Update output - optimized\n",
    "            token_str = self.tokenizer.decode([next_token], skip_special_tokens=True)\n",
    "            generated_text += token_str\n",
    "            output_ids = torch.cat([output_ids, torch.tensor([next_token], device=device)])\n",
    "            \n",
    "            # Debug output\n",
    "            if debug:\n",
    "                token_display = token_str.replace(\"\\n\", \"\\\\n\")\n",
    "                print(f\"{generation_length:4d} | {int(steering_active):8d} | {p_redundant:5.3f} | {error:5.3f} | \"\n",
    "                      f\"{alpha:5.3f} | {integral:5.3f} | {derivative:5.3f} | {temperature:5.3f} | '{token_display}'\")\n",
    "            \n",
    "            # Early stopping check - optimized\n",
    "            if self._early_stop_check(generated_text):\n",
    "                if debug:\n",
    "                    print(f\"[INFO] Stop condition met\")\n",
    "                break\n",
    "            \n",
    "            # Update progress bar\n",
    "            if not debug and hasattr(pbar, 'set_postfix'):\n",
    "                if step % 10 == 0:  # Update every 10 steps for performance\n",
    "                    pbar.set_postfix({\n",
    "                        'α': f'{alpha:.2f}', \n",
    "                        'p_red': f'{p_redundant:.2f}',\n",
    "                        'steering': steering_active\n",
    "                    })\n",
    "        \n",
    "        if not debug:\n",
    "            pbar.close()\n",
    "        \n",
    "        if debug:\n",
    "            print(\"--- END TRACE ---\")\n",
    "            print(f\"Total tokens generated: {output_ids.size(0) - input_ids.size(0)}\")\n",
    "        \n",
    "        final_text = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        tokens_generated = output_ids.size(0) - input_ids.size(0)\n",
    "        \n",
    "        return final_text, tokens_generated\n",
    "\n",
    "\n",
    "class BaselineGenerator:\n",
    "    \"\"\"Baseline generator without steering - supports both HuggingFace and vLLM backends\"\"\"\n",
    "    \n",
    "    def __init__(self, config, tokenizer, model=None, use_vllm=False):\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.use_vllm = use_vllm\n",
    "        self.vllm_engine = None\n",
    "        \n",
    "        if use_vllm:\n",
    "            self._initialize_vllm()\n",
    "    \n",
    "    def _initialize_vllm(self):\n",
    "        \"\"\"Initialize vLLM engine for faster inference\"\"\"\n",
    "        try:\n",
    "            from vllm import LLM, SamplingParams\n",
    "            \n",
    "            print(\"🚀 Initializing vLLM engine...\")\n",
    "            self.vllm_engine = LLM(\n",
    "                model=self.config.MODEL_NAME,\n",
    "                dtype=self.config.DTYPE,\n",
    "                trust_remote_code=True,\n",
    "                gpu_memory_utilization=0.8,  # Leave some GPU memory for other operations\n",
    "                max_model_len=8192,  # Adjust based on your model's context length\n",
    "                tensor_parallel_size=1,  # Adjust for multi-GPU setups\n",
    "            )\n",
    "            \n",
    "            # Pre-configure sampling parameters\n",
    "            self.sampling_params = SamplingParams(\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                max_tokens=self.config.MAX_TOKENS,\n",
    "                stop=[\"\\\\boxed{\", \"Final answer:\"],  # Stop tokens for math problems\n",
    "            )\n",
    "            \n",
    "            print(\"✅ vLLM engine initialized successfully\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"❌ vLLM not available. Install with: pip install vllm\")\n",
    "            print(\"🔄 Falling back to HuggingFace transformers\")\n",
    "            self.use_vllm = False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to initialize vLLM: {e}\")\n",
    "            print(\"🔄 Falling back to HuggingFace transformers\")\n",
    "            self.use_vllm = False\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt, max_new_tokens=None):\n",
    "        \"\"\"Generate text using either vLLM or HuggingFace backend\"\"\"\n",
    "        if self.use_vllm and self.vllm_engine:\n",
    "            return self._generate_vllm(prompt, max_new_tokens)\n",
    "        else:\n",
    "            return self._generate_hf(prompt, max_new_tokens)\n",
    "    \n",
    "    def _generate_vllm(self, prompt, max_new_tokens=None):\n",
    "        \"\"\"Generate using vLLM engine\"\"\"\n",
    "        if max_new_tokens and max_new_tokens != self.config.MAX_TOKENS:\n",
    "            # Create custom sampling params for this generation\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                max_tokens=max_new_tokens,\n",
    "                stop=[\"\\\\boxed{\", \"Final answer:\"],\n",
    "            )\n",
    "        else:\n",
    "            sampling_params = self.sampling_params\n",
    "        \n",
    "        # Generate with vLLM\n",
    "        outputs = self.vllm_engine.generate([prompt], sampling_params)\n",
    "        \n",
    "        # Extract results\n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "        tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "        \n",
    "        # Combine prompt and generated text\n",
    "        full_text = prompt + generated_text\n",
    "        \n",
    "        return full_text, tokens_generated\n",
    "    \n",
    "    def _generate_hf(self, prompt, max_new_tokens=None):\n",
    "        \"\"\"Generate using HuggingFace transformers (fallback)\"\"\"\n",
    "        if max_new_tokens is None:\n",
    "            max_new_tokens = self.config.MAX_TOKENS\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.config.DEVICE)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return generated_text, tokens_generated\n",
    "    \n",
    "    def generate_batch(self, prompts, max_new_tokens=None):\n",
    "        \"\"\"Generate for multiple prompts efficiently (vLLM only)\"\"\"\n",
    "        if not self.use_vllm or not self.vllm_engine:\n",
    "            # Fallback to sequential generation for HF\n",
    "            results = []\n",
    "            for prompt in prompts:\n",
    "                result = self._generate_hf(prompt, max_new_tokens)\n",
    "                results.append(result)\n",
    "            return results\n",
    "        \n",
    "        # Batch generation with vLLM\n",
    "        if max_new_tokens and max_new_tokens != self.config.MAX_TOKENS:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                max_tokens=max_new_tokens,\n",
    "                stop=[\"\\\\boxed{\", \"Final answer:\"],\n",
    "            )\n",
    "        else:\n",
    "            sampling_params = self.sampling_params\n",
    "        \n",
    "        outputs = self.vllm_engine.generate(prompts, sampling_params)\n",
    "        \n",
    "        results = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            generated_text = output.outputs[0].text\n",
    "            tokens_generated = len(output.outputs[0].token_ids)\n",
    "            full_text = prompts[i] + generated_text\n",
    "            results.append((full_text, tokens_generated))\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class AnswerExtractor:\n",
    "    \"\"\"Utility for extracting numerical answers from generated text\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_boxed_answer(text):\n",
    "        \"\"\"Extract the content of the last \\\\boxed{} occurrence\"\"\"\n",
    "        matches = list(re.finditer(r\"\\\\boxed\\{([^}]+)\\}\", text))\n",
    "        return matches[-1].group(1).strip() if matches else \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_reference_answer(gsm8k_answer):\n",
    "        \"\"\"Extract reference answer from GSM8K format\"\"\"\n",
    "        return gsm8k_answer.split('#### ')[1] if '#### ' in gsm8k_answer else gsm8k_answer\n",
    "\n",
    "\n",
    "class GSM8KEvaluator:\n",
    "    \"\"\"Evaluator for GSM8K mathematical reasoning tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config, raspid_generator, baseline_generator):\n",
    "        self.config = config\n",
    "        self.raspid_generator = raspid_generator\n",
    "        self.baseline_generator = baseline_generator\n",
    "        self.answer_extractor = AnswerExtractor()\n",
    "    \n",
    "    def evaluate(self, n_problems, max_tokens=None, debug=False, use_batch=True):\n",
    "        \"\"\"Evaluate both RASPID and baseline on GSM8K problems - optimized batch processing\"\"\"\n",
    "        if max_tokens is None:\n",
    "            max_tokens = self.config.MAX_TOKENS\n",
    "        \n",
    "        print(\"📚 Loading GSM8K test set...\")\n",
    "        # Load GSM8K test set (starting from problem 1000)\n",
    "        gsm8k_test = load_dataset(\"gsm8k\", \"main\")[\"test\"].select(range(1000, 1000 + n_problems))\n",
    "        \n",
    "        # Prepare prompts and examples\n",
    "        prompts = []\n",
    "        examples = []\n",
    "        for example in tqdm(gsm8k_test, desc=\"Preparing prompts\"):\n",
    "            question = example[\"question\"].strip()\n",
    "            prompt = f\"{question}\\n\\nAnswer step by step and end with: Final answer: \\\\boxed{{numeric_value}}\"\n",
    "            prompts.append(prompt)\n",
    "            examples.append(example)\n",
    "        \n",
    "        print(f\"🎯 Evaluating {len(prompts)} problems...\")\n",
    "        \n",
    "        # Phase 1: Generate ALL baseline responses first\n",
    "        print(\"🤖 Phase 1: Generating baseline responses...\")\n",
    "        if use_batch and hasattr(self.baseline_generator, 'use_vllm') and self.baseline_generator.use_vllm:\n",
    "            print(\"   Using vLLM batch generation for maximum efficiency...\")\n",
    "            baseline_results = self.baseline_generator.generate_batch(prompts, max_tokens)\n",
    "            baseline_total_tokens = sum(tokens for _, tokens in baseline_results)\n",
    "            print(f\"   ✅ Baseline batch completed: {baseline_total_tokens} total tokens\")\n",
    "        else:\n",
    "            print(\"   Using sequential generation...\")\n",
    "            baseline_results = []\n",
    "            baseline_total_tokens = 0\n",
    "            for i, prompt in enumerate(tqdm(prompts, desc=\"   Baseline generation\")):\n",
    "                result = self.baseline_generator.generate(prompt, max_tokens)\n",
    "                baseline_results.append(result)\n",
    "                baseline_total_tokens += result[1]\n",
    "                \n",
    "                # Progress update every 10 problems\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"   Progress: {i+1}/{len(prompts)} problems, {baseline_total_tokens} tokens so far\")\n",
    "        \n",
    "        # Phase 2: Generate ALL RASPID responses\n",
    "        print(\"🧠 Phase 2: Generating RASPID responses...\")\n",
    "        raspid_results = []\n",
    "        raspid_total_tokens = 0\n",
    "        \n",
    "        # Pre-warm the model (first generation is often slower)\n",
    "        if len(prompts) > 0:\n",
    "            print(\"   Pre-warming RASPID generator...\")\n",
    "            _, _ = self.raspid_generator.generate(prompts[0], min(max_tokens, 100), debug=False)\n",
    "        \n",
    "        for i, prompt in enumerate(tqdm(prompts, desc=\"   RASPID generation\")):\n",
    "            result = self.raspid_generator.generate(prompt, max_tokens, debug and i == 0)  # Debug only first\n",
    "            raspid_results.append(result)\n",
    "            raspid_total_tokens += result[1]\n",
    "            \n",
    "            # Progress update every 10 problems\n",
    "            if (i + 1) % 10 == 0:\n",
    "                avg_tokens_baseline = baseline_total_tokens // (i + 1)\n",
    "                avg_tokens_raspid = raspid_total_tokens // (i + 1)\n",
    "                efficiency = (avg_tokens_baseline - avg_tokens_raspid) / avg_tokens_baseline * 100\n",
    "                print(f\"   Progress: {i+1}/{len(prompts)} problems\")\n",
    "                print(f\"   Average tokens - Baseline: {avg_tokens_baseline}, RASPID: {avg_tokens_raspid}\")\n",
    "                print(f\"   Current efficiency: {efficiency:.1f}% token savings\")\n",
    "        \n",
    "        # Phase 3: Process and analyze results\n",
    "        print(\"📊 Phase 3: Processing results...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, example in enumerate(tqdm(examples, desc=\"   Extracting answers\")):\n",
    "            baseline_text, baseline_tokens = baseline_results[i]\n",
    "            raspid_text, raspid_tokens = raspid_results[i]\n",
    "            \n",
    "            # Extract answers\n",
    "            baseline_answer = self.answer_extractor.extract_boxed_answer(baseline_text)\n",
    "            raspid_answer = self.answer_extractor.extract_boxed_answer(raspid_text)\n",
    "            reference_answer = self.answer_extractor.extract_reference_answer(example[\"answer\"])\n",
    "            \n",
    "            results.append({\n",
    "                \"problem_id\": i,\n",
    "                \"question\": example[\"question\"],\n",
    "                \"reference_answer\": example[\"answer\"],\n",
    "                \"reference_correct\": reference_answer,\n",
    "                \"baseline_correct\": baseline_answer,\n",
    "                \"raspid_correct\": raspid_answer,\n",
    "                \"baseline_tokens\": baseline_tokens,\n",
    "                \"raspid_tokens\": raspid_tokens,\n",
    "                \"baseline_txt\": baseline_text,\n",
    "                \"raspid_txt\": raspid_text,\n",
    "                \"baseline_is_correct\": baseline_answer == reference_answer,\n",
    "                \"raspid_is_correct\": raspid_answer == reference_answer,\n",
    "            })\n",
    "        \n",
    "        # Final statistics\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        baseline_accuracy = results_df['baseline_is_correct'].mean()\n",
    "        raspid_accuracy = results_df['raspid_is_correct'].mean()\n",
    "        token_efficiency = (baseline_total_tokens - raspid_total_tokens) / baseline_total_tokens\n",
    "        avg_baseline_tokens = baseline_total_tokens / len(prompts)\n",
    "        avg_raspid_tokens = raspid_total_tokens / len(prompts)\n",
    "        \n",
    "        print(f\"\\n🎉 EVALUATION COMPLETE!\")\n",
    "        print(f\"   📈 Baseline Accuracy: {baseline_accuracy:.3f} ({results_df['baseline_is_correct'].sum()}/{len(results_df)})\")\n",
    "        print(f\"   🧠 RASPID Accuracy: {raspid_accuracy:.3f} ({results_df['raspid_is_correct'].sum()}/{len(results_df)})\")\n",
    "        print(f\"   ⚡ Token Efficiency: {token_efficiency:.3f} ({token_efficiency*100:.1f}% savings)\")\n",
    "        print(f\"   📊 Average tokens per problem:\")\n",
    "        print(f\"      Baseline: {avg_baseline_tokens:.1f}\")\n",
    "        print(f\"      RASPID: {avg_raspid_tokens:.1f}\")\n",
    "        print(f\"      Savings: {avg_baseline_tokens - avg_raspid_tokens:.1f} tokens per problem\")\n",
    "        print(f\"   📝 Total tokens:\")\n",
    "        print(f\"      Baseline: {baseline_total_tokens:,}\")\n",
    "        print(f\"      RASPID: {raspid_total_tokens:,}\")\n",
    "        print(f\"      Total savings: {baseline_total_tokens - raspid_total_tokens:,}\")\n",
    "        \n",
    "        print(\"🧠 Generating RASPID responses...\")\n",
    "        raspid_results = []\n",
    "        for prompt in tqdm(prompts, desc=\"RASPID generation\"):\n",
    "            result = self.raspid_generator.generate(prompt, max_tokens, debug)\n",
    "            raspid_results.append(result)\n",
    "        \n",
    "        # Process results\n",
    "        for i, example in enumerate(examples):\n",
    "            baseline_text, baseline_tokens = baseline_results[i]\n",
    "            raspid_text, raspid_tokens = raspid_results[i]\n",
    "            \n",
    "            # Extract answers\n",
    "            baseline_answer = self.answer_extractor.extract_boxed_answer(baseline_text)\n",
    "            raspid_answer = self.answer_extractor.extract_boxed_answer(raspid_text)\n",
    "            reference_answer = self.answer_extractor.extract_reference_answer(example[\"answer\"])\n",
    "            \n",
    "            results.append({\n",
    "                \"reference_answer\": example[\"answer\"],\n",
    "                \"reference_correct\": reference_answer,\n",
    "                \"baseline_correct\": baseline_answer,\n",
    "                \"raspid_correct\": raspid_answer,\n",
    "                \"baseline_tokens\": baseline_tokens,\n",
    "                \"raspid_tokens\": raspid_tokens,\n",
    "                \"baseline_txt\": baseline_text,\n",
    "                \"raspid_txt\": raspid_text,\n",
    "            })\n",
    "            \n",
    "            baseline_total_tokens += baseline_tokens\n",
    "            raspid_total_tokens += raspid_tokens\n",
    "        \n",
    "        print(f'📊 Total token usage - Baseline: {baseline_total_tokens}, RASPID: {raspid_total_tokens}')\n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7dc44c-db14-4c6c-b6db-dc8541449c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing RASPID...\n",
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "⚠️  Flash Attention not available: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "🔥 Applying torch.compile for potential speedup...\n",
      "✅ torch.compile applied successfully\n",
      "📚 Loading labeled data...\n",
      "   Loaded 1200 labeled examples\n",
      "   Training set: 1000 examples\n",
      "   Required thoughts: 999\n",
      "   Redundant thoughts: 998\n",
      "🎯 Training chunk classifier...\n",
      "Training chunk classifier...\n",
      "Testing chunk size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83303806641a4ae6a763547e6296eabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting embeddings:   0%|          | 0/1590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0da65096f247dd8d93cdbcb99491f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training classifier (cs=16):   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function with optimized evaluation pipeline\"\"\"\n",
    "    print(\"🚀 Initializing RASPID...\")\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = RASPIDConfig()\n",
    "    \n",
    "    # Load models and tokenizer\n",
    "    print(f\"Loading model: {config.MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n",
    "    \n",
    "    # Apply memory optimizations if enabled\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": config.DTYPE,\n",
    "        \"device_map\": \"auto\" if config.DEVICE == \"cuda\" else None\n",
    "    }\n",
    "    \n",
    "    if config.MEMORY_EFFICIENT and config.DEVICE == \"cuda\":\n",
    "        model_kwargs.update({\n",
    "            \"attn_implementation\": \"flash_attention_2\",  # Use Flash Attention if available\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.MODEL_NAME, **model_kwargs\n",
    "        ).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Flash Attention not available: {e}\")\n",
    "        # Fallback to standard attention\n",
    "        model_kwargs.pop(\"attn_implementation\", None)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.MODEL_NAME, **model_kwargs\n",
    "        ).eval()\n",
    "    \n",
    "    control_model = ControlModel(base_model, [config.EMB_LAYER])\n",
    "    \n",
    "    # Apply torch.compile for potential speedup (PyTorch 2.0+)\n",
    "    if config.ENABLE_COMPILE:\n",
    "        try:\n",
    "            import torch._dynamo\n",
    "            print(\"🔥 Applying torch.compile for potential speedup...\")\n",
    "            base_model = torch.compile(base_model, mode=\"reduce-overhead\")\n",
    "            print(\"✅ torch.compile applied successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  torch.compile not available or failed: {e}\")\n",
    "    \n",
    "    # Load and split labeled data\n",
    "    print(\"📚 Loading labeled data...\")\n",
    "    try:\n",
    "        df_all = pd.read_csv(config.LABELED_CSV)\n",
    "        print(f\"   Loaded {len(df_all)} labeled examples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Could not find {config.LABELED_CSV}\")\n",
    "        print(\"   Please ensure the labeled dataset is available\")\n",
    "        return\n",
    "    \n",
    "    df_ctrl = df_all.iloc[:1000]  # First 1000 for training\n",
    "    df_eval = df_all.iloc[1000:1200]  # Last 200 reserved for evaluation\n",
    "    \n",
    "    required_thoughts = df_ctrl[\"required_thoughts\"].fillna(\"\")\n",
    "    redundant_thoughts = df_ctrl[\"redundant_thoughts\"].fillna(\"\")\n",
    "    \n",
    "    print(f\"   Training set: {len(df_ctrl)} examples\")\n",
    "    print(f\"   Required thoughts: {len([x for x in required_thoughts if x])}\")\n",
    "    print(f\"   Redundant thoughts: {len([x for x in redundant_thoughts if x])}\")\n",
    "    \n",
    "    # Train chunk classifier\n",
    "    print(\"🎯 Training chunk classifier...\")\n",
    "    classifier_trainer = ChunkClassifier(config, tokenizer, base_model)\n",
    "    best_classifier, best_chunk_size = classifier_trainer.train(required_thoughts, redundant_thoughts)\n",
    "    \n",
    "    # Build control vector\n",
    "    print(\"🎛️  Building control vector...\")\n",
    "    vector_builder = ControlVectorBuilder(config, tokenizer, base_model)\n",
    "    control_vector = vector_builder.build_control_vector(required_thoughts, redundant_thoughts)\n",
    "    \n",
    "    # Initialize generators\n",
    "    print(\"⚙️  Initializing generators...\")\n",
    "    raspid_generator = RASPIDGenerator(\n",
    "        config, tokenizer, base_model, control_model,\n",
    "        best_classifier, best_chunk_size, control_vector\n",
    "    )\n",
    "    \n",
    "    # Initialize baseline generator with vLLM support\n",
    "    use_vllm = True  # Set to False to use HuggingFace transformers\n",
    "    baseline_generator = BaselineGenerator(config, tokenizer, base_model, use_vllm=use_vllm)\n",
    "    \n",
    "    # Evaluate on GSM8K with optimized pipeline\n",
    "    print(\"🧮 Starting GSM8K evaluation with optimized pipeline...\")\n",
    "    evaluator = GSM8KEvaluator(config, raspid_generator, baseline_generator)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results_df = evaluator.evaluate(\n",
    "        n_problems=100, \n",
    "        debug=False,  # Set to True to debug first RASPID generation\n",
    "        use_batch=use_vllm\n",
    "    )\n",
    "    \n",
    "    # Save results with timestamp\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_filename = f'results_df_100_{timestamp}.csv'\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "    print(f\"💾 Results saved to {results_filename}\")\n",
    "    \n",
    "    # Generate detailed analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 DETAILED EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    baseline_accuracy = results_df['baseline_is_correct'].mean()\n",
    "    raspid_accuracy = results_df['raspid_is_correct'].mean()\n",
    "    token_efficiency = (results_df['baseline_tokens'].sum() - results_df['raspid_tokens'].sum()) / results_df['baseline_tokens'].sum()\n",
    "    \n",
    "    print(f\"📈 Accuracy Comparison:\")\n",
    "    print(f\"   Baseline:  {baseline_accuracy:.3f} ({results_df['baseline_is_correct'].sum()}/{len(results_df)})\")\n",
    "    print(f\"   RASPID:    {raspid_accuracy:.3f} ({results_df['raspid_is_correct'].sum()}/{len(results_df)})\")\n",
    "    print(f\"   Difference: {raspid_accuracy - baseline_accuracy:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n⚡ Token Efficiency:\")\n",
    "    print(f\"   Overall savings: {token_efficiency:.3f} ({token_efficiency*100:.1f}%)\")\n",
    "    print(f\"   Avg tokens/problem:\")\n",
    "    print(f\"      Baseline: {results_df['baseline_tokens'].mean():.1f}\")\n",
    "    print(f\"      RASPID:   {results_df['raspid_tokens'].mean():.1f}\")\n",
    "    print(f\"      Savings:  {results_df['baseline_tokens'].mean() - results_df['raspid_tokens'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\n📊 Token Distribution:\")\n",
    "    print(f\"   Baseline - Min: {results_df['baseline_tokens'].min()}, Max: {results_df['baseline_tokens'].max()}, Std: {results_df['baseline_tokens'].std():.1f}\")\n",
    "    print(f\"   RASPID   - Min: {results_df['raspid_tokens'].min()}, Max: {results_df['raspid_tokens'].max()}, Std: {results_df['raspid_tokens'].std():.1f}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    problems_where_both_correct = results_df['baseline_is_correct'] & results_df['raspid_is_correct']\n",
    "    if problems_where_both_correct.any():\n",
    "        avg_savings_when_both_correct = (\n",
    "            results_df[problems_where_both_correct]['baseline_tokens'].mean() - \n",
    "            results_df[problems_where_both_correct]['raspid_tokens'].mean()\n",
    "        ) / results_df[problems_where_both_correct]['baseline_tokens'].mean()\n",
    "        print(f\"\\n🎯 When both methods are correct ({problems_where_both_correct.sum()} problems):\")\n",
    "        print(f\"   Token savings: {avg_savings_when_both_correct:.3f} ({avg_savings_when_both_correct*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🚀 Generation Method Performance:\")\n",
    "    if use_vllm and baseline_generator.use_vllm:\n",
    "        print(f\"   ✅ Baseline generation: vLLM (batch processing)\")\n",
    "    else:\n",
    "        print(f\"   🔄 Baseline generation: HuggingFace transformers (sequential)\")\n",
    "    print(f\"   🧠 RASPID generation: Custom PID-controlled (sequential)\")\n",
    "    \n",
    "    print(f\"\\n🎉 Evaluation completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
